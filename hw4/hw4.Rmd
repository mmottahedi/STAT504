---
title: "Hw4"
author: "Mohammad Mottahedi"
date: "February 6, 2016"
output: pdf_document
---
```{r, echo=FALSE}
set.seed(1234)
library(gmodels)
library(vcd)

#A function for computing Pearson correlation for IxJ tables & Mantel-Haenszel, M2
#pearson correlation for IxJ tables 
#table = IxJ table or a matrix 
#rscore=vector of row scores 
#cscore=vector of column scores 
pears.cor=function(table, rscore, cscore)
{ 
	dim=dim(table) 
	rbar=sum(margin.table(table,1)*rscore)/sum(table) 
	rdif=rscore-rbar 
	cbar=sum(margin.table(table,2)*cscore)/sum(table) 
	cdif=cscore-cbar 
	ssr=sum(margin.table(table,1)*(rdif^2)) 
	ssc=sum(margin.table(table,2)*(cdif^2)) 
	ssrc=sum(t(table*rdif)*cdif) 
	pcor=ssrc/(sqrt(ssr*ssc)) 
	pcor 
	M2=(sum(table)-1)*pcor^2
	M2
	result=c(pcor, M2)
	result
	} 
	
	
#A function for computing Spearman correlation for IxJ tables & MH statistic
#Spearman correlation for IxJ tables 
#table = IxJ table or a matrix 
#rscore=vector of midrank row scores 
#cscore=vector of midrank column scores 
spear.cor=function(table)
{ 
	table=as.table(table)
	rscore=array(data=NA, dim=dim(table)[1])
	cscore=array(data=NA, dim=dim(table)[2])
	
		for( i in 1:dim(table)[1]){
		if (i==1) rscore[i]=(margin.table(table,1)[i]+1)/2
		else 
		rscore[i]=sum(sum(margin.table(table,1)[1:(i-1)])+(margin.table(table,1)[i]+1)/2)
		}
		rscore
	    ri=rscore-sum(table)/2
	    ri=as.vector(ri)
	
		for (j in 1:dim(table)[2]){
			if (j==1) cscore[j]=(margin.table(table,2)[j]+1)/2
			else
			cscore[j]=sum(sum(margin.table(table,2)[1:(j-1)])+(margin.table(table,2)[j]+1)/2)
		}
		cscore
		ci=cscore-sum(table)/2
		ci=as.vector(ci)
				
	v=sum(t(table*ri)*ci)
	rowd=sum(table)^3-sum(margin.table(table,1)^3)
	cold=sum(table)^3-sum(margin.table(table,2)^3)
	w=(1/12)*sqrt(rowd*cold)
	
	scor=v/w
	scor
	
	M2=(sum(table)-1)*scor^2
	M2
	result=c(scor, M2)
	result
	}
	

```




__1.__ (30 pts) In a study, 1398 randomly-selected children of ages 0-15 were classified according
to whether they carried Streptococcus pyogenes and according to the size of their tonsils.

\begin{center}
Tonsil size
\end{center}

| | Normal |Slightly enlarged| Very enlarged|
|----|------|-----|------|
|Carrier| 19 |29| 24|
|Non-carrier |497| 560 |269|

(a) Analyze this table in an appropriate manner assuming that we are dealing with nominal
variables, and report relevant statistics (e.g., X2 and/or G2) and your conclusion.

```{r}
c.table <- array(data = c(19, 497, 29, 560, 24, 269),
    dim = c(2,3), dimnames = list(c("carrier", "non-carrier"), 
                                  Tonsil_size = c("normal", "slightly enlarge", "very enlarge")))
c.table


pi.hat.table <- c.table/rowSums(c.table)
pi.hat.table

result <- chisq.test(c.table, correct=F)
result

G2 <- 2 * sum(c.table * log(c.table / result$expected))
G2
# p-value for liklihood ratio
1 - pchisq(G2, 2)

```
both $\chi_2$ and $G^2$ are large with p-value less 0.05. we can reject the null hypothesis and say the variables are not independent.


(b) How many measures of association are needed to describe the table’s departure from
independence? Provide estimates and intervals for the relative risks, and interpret
the results. Why are you able to make inference about relative risks?c.table[,c(1,2)]

We need $(I-1)(J-1)=2$ measure of association to describe the tables departure from independence.

Assuming being the carrier is the response and size of tonsil is the predictor.
To summarize the relationship between the two variables we can calculate the relative risk of carriers between different tonsil sizes taking normal as the base.

```{r}
c.table <- array(data = c(19, 497, 29, 560, 24, 269),
    dim = c(2,3), dimnames = list(c("carrier", "non-carrier"),
                                  Tonsil_size = c("normal", "slightly enlarge", "very enlarge")))

p.hat.row1 <- c.table[1,] / colSums(c.table)
p.hat.row2 <- c.table[2,] / colSums(c.table)

table.1 <- array(data = c(p.hat.row1[1],p.hat.row2[1],p.hat.row1[2],p.hat.row2[2]),
    dim = c(2,2), dimnames = list(c("carrier", "non-carrier"), 
                                  Tonsil_size = c("normal", "slightly enlarge")))
table.1
```
relative risk: $\rho = \frac{P(carrier|slightly enlarge)}{P(carrier|normal)}= `r (table.1[1,2]) / (table.1[1,1]) `$

```{r}
table.2 <- array(data = c(p.hat.row1[1],p.hat.row2[1],p.hat.row1[3],p.hat.row2[3]),
    dim = c(2,2), dimnames = list(c("carrier", "non-carrier"),
                                  Tonsil_size = c("normal", "very enlarge")))
table.2
```

relative risk: $\rho =  \frac{P(carrier|very enlarge)}{P(carrier|normal)} =  `r (table.2[1,2]) / (table.2[1,1]) `$

risk being carrier increases with increasing tonsil size.


(c) Find an appropriate partitioning of the total departure from independence, as measured
by deviance (G2), for this problem. Give the sub-tables, and show that your
partitioning works. Did you learn anything more, inference wise, in comparison to
part (a).

we need $(I-1)(J-1)=2$ partitions.


```{r}
#first partition
c.table[,c(2,3)]
result <- chisq.test(c.table[,c(2,3)], correct=F)

G2.1 <- 2 * sum(result$observed * log(result$observed/ result$expected))
G2.1
# p-value for liklihood ratio
1 - pchisq(G2.1, 1)

#second partition
part.2 <- array(c(c.table[,1],c.table[,2] + c.table[,3]), dim=c(2,2))
part.2
result <- chisq.test(part.2, correct=F)

G2.2 <- 2 * sum(result$observed * log(result$observed / result$expected))
G2.2
# p-value for liklihood ratio
1 - pchisq(G2.2, 1)

#sum of two G2
G2.1 + G2.2

```


(d) Now consider ’tonsil size’ to be an ordinal variable. Re-run the analysis, if necessary,
and report the relevant statistics, your conclusions and compare to what you
got in parts (a) and (b). If you think it’s not necessary to re-run the analysis, then
explain why that’s the case.


Calculation Pearson and Spearman correlation:

```{r}
#pearson 
pears.res <- pears.cor(c.table, c(1,2), c(1,2,3))
pears.res
1 - pchisq(pears.res[2], 1)

#spearman
spear.res <- spear.cor(c.table)
spear.res
1 - pchisq(spear.res[2], 1)

```


__2.__ (25 pts) Get a dataset from
http://lib.stat.cmu.edu/DASL/Stories/EducationalAttainmentbyAge.html,
by clicking on the link after Datafile Name and input the dataset into SAS or R or other
software and perform the appropriate analysis. What interesting conclusions can you
derive about relationship between age and educational attainment?


```{r, echo=F}
data <- read.table(text = "Education	Age_Group	Count
Did not complete high school	25-34	5416
Did not complete high school	35-44	5030
Did not complete high school	45-54	5777
Did not complete high school	55-64	7606
Did not complete high school	>64	13746
Completed high school	25-34	16431
Completed high school	35-44	1855
Completed high school	45-54	9435
Completed high school	55-64	8795
Completed high school	>64	7558
College,1-3 years	25-34	8555
College,1-3 years	35-44	5576
College,1-3 years	45-54	3124
College,1-3 years	55-64	2524
College,1-3 years	>64	2503
College,4 or more years	25-34	9771
College,4 or more years	35-44	7596
College,4 or more years	45-54	3904
College,4 or more years	55-64	3109
College,4 or more years	>64	2483", sep="\t", header=T)
data
```

```{r}

table <- xtabs(data$Count ~ data$Education + data$Age_Group)
table

#Pearson 
pears.res.2 <- pears.cor(table, c(3,4,2,1), c(1,2,3,4,5))
pears.res.2 
1- pchisq(pears.res.2[2],1)

#Spearman
spear.res.2 <- spear.cor(table)
spear.res.2 
1- pchisq(spear.res.2[2],1)
```
there is strong  correlation between education and age. 



__3.__ (30 pts) In 1972, a sample of 1,524 adults reported both their current religious affiliation
and their religious affiliation at age 16.

(a) Is there any evidence of a change in the rate of Catholic affiliation over time? Find a confidence interval for the rate of change.
Current affiliation

Mcnamar test:
$H_0: \pi_{12} = \pi_{21}$
$H_A: \pi_{12} \ne \pi_{21}$

```{r}
c.table.3 <- array(data = c(351, 33, 67, 1073),
    dim = c(2,2), dimnames = list(Affiliation.at.age.16=c("catholic", "non-Catholic"), 
                                  Current_Affiliation = c("Catholic", "non-Catholic")))
c.table.3

CrossTable(x=c.table.3, mcnemar=T)

```
There's a strong evidence that rate of affiliation has changed.

```{r}
n <- sum(c.table.3)
d <- 67/sum(c.table.3) - 33/sum(c.table.3)

n12 <- c.table.3[1,2]
n21 <- c.table.3[2,1]

v_d <- 1/n * (n12/n * (1 - n12/n) + n21/n *(1 - n21/n) + 2 * n12 * n21 / n^2)

```
$\hat{d} = \pi_{12} - \pi_{21} = `r d`$

$\hat{V}(\hat{d}) = `r v_d`$

$CI = (`r d-v_d`, `r d+v_d`)$

\begin{center}
Current affiliation
\end{center}

|Affiliation at age 16| Catholic | Non-Cathotic|
|---------------|------------------|---------|
|Catholic| 351 |67|
|Non-Catholic |33 |1073|

(b) For these data, was it beneficial to record religious affiliation for the same individuals
at both points in time? In other words, could we have done just as well if we had
recorded current religious affiliation for 1,524 individuals, and religious affiliation
at age 16 for a separate independent sample of 1,524 other individuals?
```{r}
c.table.4 <- array(data = c(418, 384, 1106, 1140),
    dim = c(2,2), dimnames = list(Affiliation.at.age.16=c("catholic", "non-Catholic"), 
                                  Current_Affiliation = c("Catholic", "non-Catholic")))
c.table.4

res <-chisq.test(c.table.4, correct = F)
mcnemar.test(c.table.4, correct = F)
res



```
No, it's not beneficial the power of the cross-sectional study will have larger $\chi^2$ value.


__4.__ (15 pts) Calculate kappa for a 4 × 4 table having $n_{ii} = 5$ for all i, $n_{i,i+1} = 15, i = 1, 2, 3, n_{41} = 15$, and all other $n_{ij} = 0$. Explain why strong association does not imply
strong agreement.


```{r}
c.table.5 <- array(data = c(5,0,0,0,15,5,0,0,0,15,5,0,15,0,15,5),
    dim = c(4,4))
c.table.4

chisq.test(c.table.5)



Kappa(c.table.5)

```
It's possible to have strong negative association between to variables but in such a case there's no agreement between variables.

